{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import breizhcrops\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-governor",
   "metadata": {},
   "source": [
    "# Functions for dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "def get_datasets(datapath, mode, batchsize, preload_ram=False, level=\"L2A\"):\n",
    "    print(f\"Setting up datasets in {os.path.abspath(datapath)}, level {level}\")\n",
    "    datapath = os.path.abspath(datapath)\n",
    "\n",
    "    frh01 = breizhcrops.BreizhCrops(region=\"frh01\", root=datapath,\n",
    "                                    preload_ram=preload_ram, level=level)\n",
    "    frh02 = breizhcrops.BreizhCrops(region=\"frh02\", root=datapath,\n",
    "                                    preload_ram=preload_ram, level=level)\n",
    "    frh03 = breizhcrops.BreizhCrops(region=\"frh03\", root=datapath,\n",
    "                                    preload_ram=preload_ram, level=level)\n",
    "    if not \"validation_only\" in mode:\n",
    "            frh04 = breizhcrops.BreizhCrops(region=\"frh04\", root=datapath,\n",
    "                                            preload_ram=preload_ram, level=level)\n",
    "\n",
    "    if mode == \"evaluation\" or mode == \"evaluation1\":\n",
    "        traindatasets = torch.utils.data.ConcatDataset([frh01, frh02, frh03])\n",
    "        testdataset = frh04\n",
    "    elif mode == \"validation_only\":\n",
    "        traindatasets = torch.utils.data.ConcatDataset([frh01, frh02])\n",
    "        validationdataset = frh03\n",
    "    elif mode == \"validation_test\":\n",
    "        traindatasets = torch.utils.data.ConcatDataset([frh01, frh02])\n",
    "        validationdataset = frh03\n",
    "        testdataset = frh04\n",
    "        \n",
    "    elif mode == 'all_zones':\n",
    "        traindatasets = frh01\n",
    "        testdataset1 = frh02\n",
    "        testdataset2 = frh03        \n",
    "        testdataset3 = frh04\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"only --mode 'validation' or 'evaluation' allowed\")\n",
    "    meta = dict(\n",
    "        ndims=13 if level==\"L1C\" else 10,\n",
    "        num_classes=len(frh01.classes),\n",
    "        sequencelength=45\n",
    "    )\n",
    "\n",
    "    return traindatasets, testdataset1, testdataset2, testdataset3, meta   \n",
    "\n",
    "def get_dataloader(traindatasets,testdataset, validationdataset=None, batchsize=32, workers=0):\n",
    "    traindataloader = DataLoader(traindatasets, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "    validationdataloader = DataLoader(validationdataset, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "    testdataloader = DataLoader(testdataset, batch_size=batchsize, shuffle=False, num_workers=workers)\n",
    "\n",
    "    return traindataloader, validationdataloader, testdataloader\n",
    "\n",
    "def get_dataloader2(traindatasets,testdataset1, testdataset2, testdataset3, batchsize=32, workers=0):\n",
    "    traindataloader = DataLoader(traindatasets, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "    testdataloader1 = DataLoader(testdataset1, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "    testdataloader2 = DataLoader(testdataset2, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "    testdataloader3 = DataLoader(testdataset3, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "\n",
    "    return traindataloader, testdataloader1, testdataloader2, testdataloader3\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred, training = False):\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(y_true, y_pred)\n",
    "    f1_micro = sklearn.metrics.f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro = sklearn.metrics.f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1_weighted = sklearn.metrics.f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    recall_micro = sklearn.metrics.recall_score(y_true, y_pred, average=\"micro\")\n",
    "    recall_macro = sklearn.metrics.recall_score(y_true, y_pred, average=\"macro\")\n",
    "    recall_weighted = sklearn.metrics.recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    precision_micro = sklearn.metrics.precision_score(y_true, y_pred, average=\"micro\")\n",
    "    precision_macro = sklearn.metrics.precision_score(y_true, y_pred, average=\"macro\")\n",
    "    precision_weighted = sklearn.metrics.precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    \n",
    "    if training == True:\n",
    "        return accuracy\n",
    "    \n",
    "    return dict(\n",
    "        accuracy=accuracy,\n",
    "        kappa=kappa,\n",
    "        f1_micro=f1_micro,\n",
    "        f1_macro=f1_macro,\n",
    "        f1_weighted=f1_weighted,\n",
    "        recall_micro=recall_micro,\n",
    "        recall_macro=recall_macro,\n",
    "        recall_weighted=recall_weighted,\n",
    "        precision_micro=precision_micro,\n",
    "        precision_macro=precision_macro,\n",
    "        precision_weighted=precision_weighted,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-fiber",
   "metadata": {},
   "source": [
    "# Define params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = 4\n",
    "num_classes=9\n",
    "d_model=64\n",
    "n_head=2\n",
    "n_layers=3\n",
    "d_inner=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('breizhcrops_dataset')\n",
    "mode = 'all_zones'\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "\n",
    "weight_decay = 5e-08\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-conducting",
   "metadata": {},
   "source": [
    "# Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1dataset, zone2dataset, zone3dataset, zone4dataset, meta = get_datasets(datapath=datapath, mode=mode,\n",
    "                                                        batchsize=batch_size, \n",
    "                                                        preload_ram=True, level=\"L2A\")\n",
    "num_classes = meta[\"num_classes\"]\n",
    "ndims = meta[\"ndims\"]\n",
    "sequencelength = meta[\"sequencelength\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1dataloader, zone2dataloader, zone3dataloader, zone4dataloader = get_dataloader2(zone1dataset, zone2dataset, \n",
    "                                                                                     zone3dataset, zone4dataset,\n",
    "                                                                                     batchsize=batch_size, \n",
    "                                                                                     workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-mainland",
   "metadata": {},
   "source": [
    "# Define DANN Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.transformer import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn.modules import LayerNorm, Linear, Sequential, ReLU, GELU\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "__all__ = ['ViTransformerExtractor']\n",
    "\n",
    "class ViTransformerExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=10, num_classes=9, time_dim = 45, d_model=64, n_head=2, n_layers=5,\n",
    "                 d_inner=128, activation=\"relu\", dropout=0.1):\n",
    "\n",
    "        super(ViTransformerExtractor, self).__init__()\n",
    "        self.modelname = f\"TransformerEncoder_input-dim={input_dim}_num-classes={num_classes}_\" \\\n",
    "                         f\"d-model={d_model}_d-inner={d_inner}_n-layers={n_layers}_n-head={n_head}_\" \\\n",
    "                         f\"dropout={dropout}\"\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, n_head, d_inner, dropout, activation)\n",
    "        encoder_norm = LayerNorm(d_model)\n",
    "\n",
    "        self.inlinear = Linear(input_dim, d_model)\n",
    "        self.relu = ReLU()\n",
    "        self.transformerencoder = TransformerEncoder(encoder_layer, n_layers, encoder_norm)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, time_dim, d_model)) # T + class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model)) # class token\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.n_units  = 128\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.inlinear(x) # B x T x D\n",
    "\n",
    "        b, n, _ = x.shape # B x T x D\n",
    "\n",
    "        #cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)  # repeat for all batch\n",
    "        #x = torch.cat((cls_tokens, x), dim=1) # concatenate on sequence [T + class token]\n",
    "        x += self.pos_embedding[:, :(n)]\n",
    "        \n",
    "        x = x.transpose(0, 1) # N x T x D -> T x N x D\n",
    "        x = self.transformerencoder(x)\n",
    "        x = x.transpose(0, 1) # T x N x D -> N x T x D\n",
    "\n",
    "        features = x.max(1)[0]  # take first dimension B x T x D\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ex = ViTransformerExtractor(input_dim=ndims, n_head = n_head, n_layers = n_layers, activation=\"relu\",).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DANN model: an additional fully-connected branch is added for the domain classifier\n",
    "from torch.nn.modules.transformer import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn.modules import LayerNorm, Linear, Sequential, ReLU\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Function\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    # Forwards identity\n",
    "    # Sends backward reversed gradients\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "__all__ = ['ViTransformerDANN']\n",
    "\n",
    "class ViTransformerDANN(nn.Module):\n",
    "    def __init__(self, feature_ex, input_dim=13, num_classes=9, d_model=64, n_head=2, n_layers=5, n_domain=2,\n",
    "                 d_inner=128, activation=\"relu\", dropout=0.1):\n",
    "\n",
    "        super(ViTransformerDANN, self).__init__()\n",
    "        self.modelname = f\"TransformerEncoder_input-dim={input_dim}_num-classes={num_classes}_\" \\\n",
    "                         f\"d-model={d_model}_d-inner={d_inner}_n-layers={n_layers}_n-head={n_head}_\" \\\n",
    "                         f\"dropout={dropout}\"\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, n_head, d_inner, dropout, activation)\n",
    "        encoder_norm = LayerNorm(d_model)\n",
    "\n",
    "        self.inlinear = Linear(input_dim, d_model)\n",
    "        self.relu = ReLU()\n",
    "        self.transformerencoder = TransformerEncoder(encoder_layer, n_layers, encoder_norm)\n",
    "        self.n_units  = 128\n",
    "        self.fc1 = Linear(d_model, self.n_units)\n",
    "        self.fc2 = Linear(d_model, self.n_units)\n",
    "        self.outlinear = Linear(self.n_units, num_classes)\n",
    "        \n",
    "        self.dropout_p = 0.1\n",
    "        self.dropout = nn.Dropout(p = self.dropout_p)\n",
    "        self.outlinear_dom = Linear(self.n_units, n_domain)\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            LayerNorm(d_model),\n",
    "            Linear(d_model, self.n_units),\n",
    "            ReLU(),\n",
    "            nn.Dropout(p = self.dropout_p),\n",
    "            Linear(self.n_units, num_classes)\n",
    "        )\n",
    "\n",
    "        self.features = feature_ex\n",
    "    \n",
    "    def forward(self, x, alpha=None):\n",
    "        embeddings = self.features(x)\n",
    "\n",
    "        # If we pass alpha, we can assume we are training the discriminator\n",
    "        if alpha is not None:\n",
    "            # gradient reversal layer (backward gradients will be reversed)\n",
    "            reverse_feature = ReverseLayerF.apply(embeddings, alpha)\n",
    "            x = self.fc2(reverse_feature)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            domain_output = self.outlinear_dom(x)\n",
    "            return domain_output\n",
    "\n",
    "        # If we don't pass alpha, we assume we are training with supervision\n",
    "        else:\n",
    "            # pass features to labels classifier\n",
    "            class_logits = self.mlp_head(embeddings)\n",
    "\n",
    "            return embeddings, class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "dann_model = ViTransformerDANN(feature_ex, input_dim=ndims, num_classes=num_classes, n_layers = n_layers, \n",
    "                                n_domain=2,\n",
    "                                activation=\"relu\",).to(device)\n",
    "\n",
    "dann_model.modelname += f\"_learning-rate={learning_rate}_weight-decay={weight_decay}\"\n",
    "print(f\"Initialized {dann_model.modelname}\")\n",
    "\n",
    "logdir = 'logs/torch_transformer_dann/'\n",
    "\n",
    "print(f\"Initialized {dann_model.modelname}\")\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "print(f\"Logging results to {logdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-cloud",
   "metadata": {},
   "source": [
    "# Prepare Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dann_model, optimizer, criterion, traindataloader, testdataloader, device, \n",
    "                running_loss1, running_loss2, running_loss3, alpha):\n",
    "    \n",
    "    dann_model.train()\n",
    "    losses = list()\n",
    "    y_true_list = list()\n",
    "    y_pred_list = list()\n",
    "    \n",
    "    dataloader_iterator = iter(testdataloader)\n",
    "    \n",
    "    with tqdm(enumerate(traindataloader), total=len(traindataloader), leave=True) as iterator:\n",
    "        for idx, batch in iterator:\n",
    "            optimizer.zero_grad()\n",
    "            x, y_true, _ = batch\n",
    "            x = x.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "            \n",
    "            embds, logits = dann_model.forward(x)\n",
    "            Gy_loss = criterion(logits, y_true)\n",
    "            running_loss1 += Gy_loss.item()\n",
    "            Gy_loss.backward()\n",
    "            \n",
    "            y_true_list.append(y_true)\n",
    "            y_pred_list.append(logits.argmax(-1))\n",
    "            iterator.set_description(f\"train loss={Gy_loss:.2f}\")\n",
    "            losses.append(Gy_loss)\n",
    "            \n",
    "            #2 - train Gd on source domain data, the label is zero for all samples\n",
    "            #Forward pass to the Gd discriminator\n",
    "            Gd_outputs_source = dann_model(x, alpha)\n",
    "            domain_labels0 = torch.zeros(y_true.size(0), dtype=torch.int64).to(device)\n",
    "            Gd_loss_s = criterion(Gd_outputs_source, domain_labels0)\n",
    "            running_loss2 += Gd_loss_s.item()\n",
    "            Gd_loss_s.backward()  # backward pass: computes gradients\n",
    "\n",
    "            #3 - train discriminator Gd on target domain data, the label is 1 for all samples\n",
    "            #iterate batch from target domain dataloader\n",
    "            try:\n",
    "                batch = next(dataloader_iterator)\n",
    "                x, y_true, _ = batch\n",
    "          \n",
    "            except StopIteration:\n",
    "                dataloader_iterator = iter(testdataloader)\n",
    "                batch = next(dataloader_iterator)\n",
    "                x, y_true, _ = batch\n",
    "\n",
    "            x = x.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "            #Forward pass to Gd with target domain data\n",
    "            Gd_outputs_target = dann_model(x, alpha)\n",
    "            domain_labels1 = torch.ones(y_true.size(0), dtype=torch.int64).to(device)\n",
    "            Gd_loss_t = criterion(Gd_outputs_target, domain_labels1)\n",
    "            running_loss3 += Gd_loss_t.item()\n",
    "            Gd_loss_t.backward()  # backward pass: computes gradients\n",
    "\n",
    "            optimizer.step() # update weights based on accumulated gradients\n",
    "\n",
    "    return running_loss1, running_loss2, running_loss3, torch.stack(losses), torch.cat(y_true_list), torch.cat(y_pred_list)\n",
    "\n",
    "\n",
    "def test_epoch(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = list()\n",
    "        y_true_list = list()\n",
    "        y_pred_list = list()\n",
    "        y_score_list = list()\n",
    "        field_ids_list = list()\n",
    "        with tqdm(enumerate(dataloader), total=len(dataloader), leave=True) as iterator:\n",
    "            for idx, batch in iterator:\n",
    "                x, y_true, field_id = batch\n",
    "                embds, logits = model.forward(x.to(device))\n",
    "                loss = criterion(logits, y_true.to(device))\n",
    "                iterator.set_description(f\"test loss={loss:.2f}\")\n",
    "                losses.append(loss)\n",
    "                y_true_list.append(y_true)\n",
    "                y_pred_list.append(logits.argmax(-1))\n",
    "                y_score_list.append(logits.exp())\n",
    "                field_ids_list.append(field_id)\n",
    "        return torch.stack(losses), torch.cat(y_true_list), torch.cat(y_pred_list), torch.cat(y_score_list), torch.cat(field_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = 250\n",
    "alpha = np.empty(epochs)\n",
    "p = np.linspace(0.0,1.0,epochs)\n",
    "\n",
    "gammas = [4,5,8,10]\n",
    "for gamma in gammas:\n",
    "    alpha = 0.5*(2/(1+np.exp(-gamma*p)) - 1)\n",
    "\n",
    "    plt.plot(range(epochs), alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-regard",
   "metadata": {},
   "source": [
    "# Training loop DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = 4\n",
    "dataloaders = [(zone1dataloader,'zone1'), (zone2dataloader,'zone2'), (zone3dataloader,'zone3'), (zone4dataloader,'zone4')]\n",
    "epochs = 250\n",
    "\n",
    "#set domain adaptation parameter schedule\n",
    "p = np.linspace(0.0,1.0,epochs)\n",
    "gamma = 10\n",
    "alphas = 0.2*(2/(1+np.exp(-gamma*p)) - 1)\n",
    "\n",
    "# set constant alpha\n",
    "alpha_c = 0.2\n",
    "#alphas = alpha_c * np.ones(epochs, dtype = np.float32)\n",
    "\n",
    "save_freq = 10\n",
    "\n",
    "for i in range(zones):\n",
    "    \n",
    "    traindataloader = dataloaders[i][0]\n",
    "    train_zone = dataloaders[i][1]\n",
    "    print('Training zone:', train_zone)\n",
    "    \n",
    "    for j in range(zones):\n",
    "        if j!= i:\n",
    "            # DEFINE MODE DIR AND NAME\n",
    "                # if alpha scheduled save model as:\n",
    "            path = 'models/vio_trasformer_dann_s'+str(i+1)+'_t'+ str(j+1)+'_maxalpha02_gamma'+str(gamma)\n",
    "                # if constant alpha save model as:\n",
    "            #path = 'models/vio_trasformer_dann_s'+str(i+1)+'_t'+ str(j+1)+'_alpha'+str(alpha_c)\n",
    "            model_dir = Path(path)\n",
    "            \n",
    "            # DEFINE MODEL\n",
    "            feature_ex = ViTransformerExtractor(input_dim=ndims, n_head = n_head, n_layers = n_layers, activation=\"relu\",).to(device)\n",
    "            dann_model = ViTransformerDANN(feature_ex, input_dim=ndims, num_classes=num_classes,\n",
    "                                            n_layers = n_layers, \n",
    "                                            n_domain=2,\n",
    "                                            activation=\"relu\",).to(device)\n",
    "\n",
    "            dann_model.modelname += f\"_learning-rate={learning_rate}_weight-decay={weight_decay}\"\n",
    "            print(f\"Initialized {dann_model.modelname}\")\n",
    "            \n",
    "            # SET LOSS\n",
    "            criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "            # SET OPTIMIZER AND LR SCHEDULER\n",
    "            optimizer = Adam(dann_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            lambda2 = lambda epoch: 0.99 ** epoch\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda2)\n",
    "            \n",
    "            # SET TARGET DATA\n",
    "            testdataloader = dataloaders[j][0]\n",
    "            test_zone = dataloaders[j][1]\n",
    "            print('Testing zone:', test_zone)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                alpha = alphas[epoch]\n",
    "\n",
    "                running_loss1 = 0.0\n",
    "                running_loss2 = 0.0\n",
    "                running_loss3 = 0.0\n",
    "\n",
    "                running_loss1, running_loss2, running_loss3, train_loss, y_true, y_pred  = train_epoch(dann_model, optimizer, criterion,\n",
    "                                                                        traindataloader,testdataloader, device, \n",
    "                                                                        running_loss1, running_loss2, running_loss3, alpha)\n",
    "                loss_step = running_loss1 / len(traindataloader)\n",
    "                loss_Gd_source = running_loss2 / len(traindataloader)\n",
    "                loss_Gd_target = running_loss3 / len(testdataloader)\n",
    "\n",
    "                train_scores = metrics(y_true.cpu(), y_pred.cpu())\n",
    "\n",
    "                # Record loss and accuracy into the writer for training\n",
    "               #writer.add_scalar('Train Gy/Loss', loss_step, epoch)\n",
    "                train_accuracy = train_scores[\"accuracy\"]\n",
    "                #writer.add_scalar('Train Gy/Accuracy', train_accuracy, epoch)\n",
    "                #writer.add_scalar('Train Gd source/Loss', loss_Gd_source, epoch)\n",
    "                #writer.add_scalar('Train Gd source/Loss', loss_Gd_target, epoch)\n",
    "                #writer.flush()\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss = train_loss.cpu().detach().numpy()[0]\n",
    "            #    print(f\"epoch {epoch}: trainloss {train_loss:.2f}, testloss {test_loss:.2f} \" + scores_msg)\n",
    "                print(f\"epoch {epoch}: trainloss {train_loss:.2f} \")\n",
    "\n",
    "                if epoch % save_freq == 0:\n",
    "                    torch.save(dann_model.state_dict(), model_dir)\n",
    "            torch.save(dann_model.state_dict(), model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-freeze",
   "metadata": {},
   "source": [
    "# Test on all zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = 4\n",
    "dataloaders = [(zone1dataloader,'zone1'), (zone2dataloader,'zone2'), (zone3dataloader,'zone3'), (zone4dataloader,'zone4')]\n",
    "\n",
    "# DEFINE MODEL\n",
    "feature_ex = ViTransformerExtractor(input_dim=ndims, n_head = n_head, n_layers = n_layers, activation=\"relu\",).to(device)\n",
    "dann_model = ViTransformerDANN(feature_ex, input_dim=ndims, num_classes=num_classes,\n",
    "                                n_layers = n_layers, n_domain=2,activation=\"relu\",).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "for i in range(zones):\n",
    "    \n",
    "    traindataloader = dataloaders[i][0]\n",
    "    train_zone = dataloaders[i][1]\n",
    "    print('Source zone:', train_zone)\n",
    "    \n",
    "    for j in range(zones):\n",
    "        if j!= i:\n",
    "            # DEFINE MODE DIR AND NAME\n",
    "                # if alpha scheduled save model as:\n",
    "            path = 'models/vio_trasformer_dann_s'+str(i+1)+'_t'+ str(j+1)+'_maxalpha02_gamma'+str(gamma)\n",
    "                # if constant alpha save model as:\n",
    "            #path = 'models/vio_trasformer_dann_s'+str(i+1)+'_t'+ str(j+1)+'_alpha'+str(alpha_c)\n",
    "            model_dir = Path(path)\n",
    "            dann_model.load_state_dict(torch.load(model_dir))\n",
    "            \n",
    "            # SET TARGET DATA\n",
    "            testdataloader = dataloaders[j][0]\n",
    "            test_zone = dataloaders[j][1]\n",
    "            print('Target zone:', test_zone)\n",
    "\n",
    "            test_loss, y_true, y_pred, *_ = test_epoch(dann_model, criterion, testdataloader, device)\n",
    "\n",
    "            scores = metrics(y_true.cpu(), y_pred.cpu())\n",
    "            scores_msg = \", \".join([f\"{k}={v:.2f}\" for (k, v) in scores.items()])\n",
    "            test_loss = test_loss.cpu().detach().numpy()[0]\n",
    "            print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-glucose",
   "metadata": {},
   "source": [
    "# Test on single Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = 4\n",
    "dataloaders = [(zone1dataloader,'zone1'), (zone2dataloader,'zone2'), (zone3dataloader,'zone3'), (zone4dataloader,'zone4')]\n",
    "feature_ex = ViTransformerExtractor(input_dim=ndims, n_head = n_head, n_layers = n_layers, activation=\"relu\",).to(device)\n",
    "dann_model = ViTransformerDANN(feature_ex, input_dim=ndims, num_classes=num_classes,\n",
    "                                n_layers = n_layers, n_domain=2,activation=\"relu\",).to(device)\n",
    "\n",
    "path = 'models/vio_trasformer_dann_s2_t3_maxalpha02_gamma10'\n",
    "model_dir = Path(path)\n",
    "dann_model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# SET TARGET DATA\n",
    "ZONE = 3\n",
    "testdataloader = dataloaders[ZONE-1][0]\n",
    "test_zone = dataloaders[ZONE-1][1]\n",
    "print('Target zone:', test_zone)\n",
    "\n",
    "test_loss, y_true, y_pred, *_ = test_epoch(dann_model, criterion, testdataloader, device)\n",
    "\n",
    "scores = metrics(y_true.cpu(), y_pred.cpu())\n",
    "scores_msg = \", \".join([f\"{k}={v:.2f}\" for (k, v) in scores.items()])\n",
    "\n",
    "test_loss = test_loss.cpu().detach().numpy()[0]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-elder",
   "metadata": {},
   "source": [
    "#  Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_names = ['barley', 'wheat', 'rapeseed', 'corn', 'sunflower', 'orchards',\n",
    "       'nuts', 'permanent_meadows', 'temporary_meadows']\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, num_classes=9, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    matplotlib.rc('xtick', labelsize=16) \n",
    "    matplotlib.rc('ytick', labelsize=16) \n",
    "    plt.title(title)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize='large')\n",
    "    plt.xlabel('Predicted label', fontsize='large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true.cpu(), y_pred.cpu())\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, label_names, title = 'Confusion matrix zone 3: DANN')\n",
    "\n",
    "fig.savefig('feature_visualization/DANNconfusion_matrix_zone2_3.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
