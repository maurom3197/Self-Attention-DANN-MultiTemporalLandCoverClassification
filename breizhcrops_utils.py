{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "def get_datasets(datapath, mode, batchsize, preload_ram=False, level=\"L2A\"):\n",
    "    print(f\"Setting up datasets in {os.path.abspath(datapath)}, level {level}\")\n",
    "    datapath = os.path.abspath(datapath)\n",
    "\n",
    "    frh01 = breizhcrops.BreizhCrops(region=\"frh01\", root=datapath,\n",
    "                                    preload_ram=preload_ram, level=level)\n",
    "    frh02 = breizhcrops.BreizhCrops(region=\"frh02\", root=datapath,\n",
    "                                    preload_ram=preload_ram, level=level)\n",
    "    frh03 = breizhcrops.BreizhCrops(region=\"frh03\", root=datapath,\n",
    "                                    preload_ram=preload_ram, level=level)\n",
    "    if not \"validation_only\" in mode:\n",
    "            frh04 = breizhcrops.BreizhCrops(region=\"frh04\", root=datapath,\n",
    "                                            preload_ram=preload_ram, level=level)\n",
    "\n",
    "    if mode == \"evaluation\" or mode == \"evaluation1\":\n",
    "        traindatasets = torch.utils.data.ConcatDataset([frh01, frh02, frh03])\n",
    "        testdataset = frh04\n",
    "    elif mode == \"evaluation2\":\n",
    "        traindatasets = torch.utils.data.ConcatDataset([frh01, frh02, frh04])\n",
    "        testdataset = frh03\n",
    "    elif mode == \"evaluation3\":\n",
    "        traindatasets = torch.utils.data.ConcatDataset([frh01, frh03, frh04])\n",
    "        testdataset = frh02\n",
    "    elif mode == \"evaluation4\":\n",
    "        traindatasets = torch.utils.data.ConcatDataset([frh02, frh03, frh04])\n",
    "        testdataset = frh01\n",
    "    elif mode == \"validation_only\":\n",
    "        traindatasets = torch.utils.data.ConcatDataset([frh01, frh02])\n",
    "        validationdataset = frh03\n",
    "    elif mode == \"validation_test\":\n",
    "        traindatasets = torch.utils.data.ConcatDataset([frh01, frh02])\n",
    "\n",
    "        validationdataset = frh03\n",
    "        testdataset = frh04\n",
    "        \n",
    "    elif mode == 'train1':\n",
    "        traindatasets = frh01\n",
    "        testdataset1 = frh02\n",
    "        testdataset2 = frh03        \n",
    "        testdataset3 = frh04\n",
    "    elif mode == 'train2':\n",
    "        traindatasets = frh02\n",
    "        testdataset1 = frh01\n",
    "        testdataset2 = frh03        \n",
    "        testdataset3 = frh04\n",
    "    elif mode == 'train3':\n",
    "        traindatasets = frh03\n",
    "        testdataset1 = frh01\n",
    "        testdataset2 = frh02        \n",
    "        testdataset3 = frh04  \n",
    "    elif mode == 'train4':\n",
    "        traindatasets = frh04\n",
    "        testdataset1 = frh01\n",
    "        testdataset2 = frh02        \n",
    "        testdataset3 = frh03\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"only --mode 'validation' or 'evaluation' allowed\")\n",
    "    meta = dict(\n",
    "        ndims=13 if level==\"L1C\" else 10,\n",
    "        num_classes=len(frh01.classes),\n",
    "        sequencelength=45\n",
    "    )\n",
    "\n",
    "    return traindatasets, testdataset1, testdataset2, testdataset3, meta   \n",
    "\n",
    "def get_generic_dataloader(traindataset, batchsize=256, workers=4):\n",
    "    dataloader = DataLoader(traindataset, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def get_dataloader(traindatasets,testdataset, validationdataset=None, batchsize=32, workers=4):\n",
    "    traindataloader = DataLoader(traindatasets, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "    validationdataloader = DataLoader(validationdataset, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "    testdataloader = DataLoader(testdataset, batch_size=batchsize, shuffle=False, num_workers=workers)\n",
    "\n",
    "    return traindataloader, validationdataloader, testdataloader\n",
    "\n",
    "def get_dataloader2(traindatasets,testdataset1, testdataset2, testdataset3, batchsize=32, workers=4):\n",
    "    traindataloader = DataLoader(traindatasets, batch_size=batchsize, shuffle=True, num_workers=workers)\n",
    "    testdataloader1 = DataLoader(testdataset1, batch_size=batchsize, shuffle=False, num_workers=workers)\n",
    "    testdataloader2 = DataLoader(testdataset2, batch_size=batchsize, shuffle=False, num_workers=workers)\n",
    "    testdataloader3 = DataLoader(testdataset3, batch_size=batchsize, shuffle=False, num_workers=workers)\n",
    "\n",
    "    return traindataloader, testdataloader1, testdataloader2, testdataloader3\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred, training = False):\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(y_true, y_pred)\n",
    "    f1_micro = sklearn.metrics.f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro = sklearn.metrics.f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1_weighted = sklearn.metrics.f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    recall_micro = sklearn.metrics.recall_score(y_true, y_pred, average=\"micro\")\n",
    "    recall_macro = sklearn.metrics.recall_score(y_true, y_pred, average=\"macro\")\n",
    "    recall_weighted = sklearn.metrics.recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    precision_micro = sklearn.metrics.precision_score(y_true, y_pred, average=\"micro\")\n",
    "    precision_macro = sklearn.metrics.precision_score(y_true, y_pred, average=\"macro\")\n",
    "    precision_weighted = sklearn.metrics.precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    \n",
    "    if training == True:\n",
    "        return accuracy\n",
    "    \n",
    "    return dict(\n",
    "        accuracy=accuracy,\n",
    "        kappa=kappa,\n",
    "        f1_micro=f1_micro,\n",
    "        f1_macro=f1_macro,\n",
    "        f1_weighted=f1_weighted,\n",
    "        recall_micro=recall_micro,\n",
    "        recall_macro=recall_macro,\n",
    "        recall_weighted=recall_weighted,\n",
    "        precision_micro=precision_micro,\n",
    "        precision_macro=precision_macro,\n",
    "        precision_weighted=precision_weighted,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, dataloader, device, running_loss):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "    y_true_list = list()\n",
    "    y_pred_list = list()\n",
    "    with tqdm(enumerate(dataloader), total=len(dataloader), leave=True) as iterator:\n",
    "        for idx, batch in iterator:\n",
    "            optimizer.zero_grad()\n",
    "            x, y_true, _ = batch\n",
    "\n",
    "            emb, logits, logprobabilities = model.forward(x.to(device))\n",
    "            loss = criterion(logits, y_true.to(device))\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_true_list.append(y_true)\n",
    "            y_pred_list.append(logits.argmax(-1))\n",
    "            iterator.set_description(f\"train loss={loss:.2f}\")\n",
    "            losses.append(loss)\n",
    "            \n",
    "            \n",
    "    return running_loss, torch.stack(losses), torch.cat(y_true_list), torch.cat(y_pred_list)\n",
    "\n",
    "\n",
    "def test_epoch(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = list()\n",
    "        y_true_list = list()\n",
    "        y_pred_list = list()\n",
    "        y_score_list = list()\n",
    "        field_ids_list = list()\n",
    "        with tqdm(enumerate(dataloader), total=len(dataloader), leave=True) as iterator:\n",
    "            for idx, batch in iterator:\n",
    "                x, y_true, field_id = batch\n",
    "                embs, logits, logprobabilities = model.forward(x.to(device))\n",
    "                loss = criterion(logits, y_true.to(device))\n",
    "                iterator.set_description(f\"test loss={loss:.2f}\")\n",
    "                losses.append(loss)\n",
    "                y_true_list.append(y_true)\n",
    "                y_pred_list.append(logits.argmax(-1))\n",
    "                y_score_list.append(logits.exp())\n",
    "                field_ids_list.append(field_id)\n",
    "        return torch.stack(losses), torch.cat(y_true_list), torch.cat(y_pred_list), torch.cat(y_score_list), torch.cat(field_ids_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
